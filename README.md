# ðŸ§  NanoGo: Lightweight Deep Learning for the Game of Go

**NanoGo** is a deep learning project focused on training compact neural networks (â‰¤100k parameters) to play the game of Go using 1,000,000 self-play games generated by **KataGo**.  
The project explores various modern architectures optimized for efficiency: ResNet, MobileNet, Inception+CBAM, MNASNet, Linformer, EfficientFormer, and more.

---


## ðŸ“ Project Structure

```
NanoGo/
â”œâ”€â”€ assets/                    # Architecture diagrams (used in report)
â”‚   â”œâ”€â”€ dyt.png
â”‚   â”œâ”€â”€ efficientformer.jpg
â”‚   â”œâ”€â”€ hybridlinformer.jpg
â”‚   â”œâ”€â”€ inception.jpg
â”‚   â”œâ”€â”€ mnasnet.jpg
â”‚   â”œâ”€â”€ mobile+cbam.jpg
â”‚   â”œâ”€â”€ mobilenet.jpg
â”‚   â”œâ”€â”€ resnet.jpg
â”‚   â””â”€â”€ transformer_dyt.jpg
â”œâ”€â”€ data/                     
â”œâ”€â”€ libs/                      # Compiled pybind11 shared objects (golois*.so)
â”‚   â”œâ”€â”€ golois.cpython-37m-*.so
â”‚   â”œâ”€â”€ Golois.cpython-38-*.so
â”‚   â”œâ”€â”€ Golois.cpython-310-*.so
â”‚   â””â”€â”€ golois.cpython-311-*.so
â”œâ”€â”€ models/                    # Architecture definitions (no training here)
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ notebook_final.ipynb   # Main training & evaluation notebook
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ logs/                  # Training logs or CSVs
â”‚   â””â”€â”€ plot/                  # Generated plots (e.g. learning curves)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ networks/              # Modular layers, SE blocks, etc.
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ trainGolois.py      
â”œâ”€â”€ report.pdf                 # Final report with figures and conclusions
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

##  Installation & Execution on macOS (âš ï¸ Apple Silicon specific)

Some macOS versions may have issues with the default `clang++`. Here's the recommended approach:

### 1. Install LLVM
```bash
brew install llvm
```

### 2. Compile `golois` (via pybind11)
Inside the folder containing `golois.cpp`, run:

```bash
/opt/homebrew/opt/llvm/bin/clang++ -O3 -Wall -shared -std=c++11 -undefined dynamic_lookup \
$(python3 -m pybind11 --includes) golois.cpp -o golois$(python3-config --extension-suffix)
```

> This will generate a `golois*.so` file that can be imported in Python.

---

## Dependencies

Install all required Python packages:

```bash
pip install -r requirements.txt
```

---

## Training & Visualization

The `models/` folder only contains architecture definitions.  
All training, evaluation, and visualization are conducted inside the main notebook:

```
notebooks/notebook_final.ipynb
```

Training metrics (loss, accuracy, MSE) are automatically saved in the `metrics/` directory.

---

## Report & Visual Results

You can find the full report in `report.pdf`, including:

- Architecture diagrams (also available in `assets/`)
- Performance comparison (accuracy, MSE, win rate)
- Discussion of design decisions
- Round-robin tournament results

---

## References

- Silver et al., [Mastering the Game of Go with Deep Neural Networks and Tree Search](https://www.nature.com/articles/nature16961), *Nature*, 2016  
- Silver et al., [A General Reinforcement Learning Algorithm that Masters Chess, Shogi, and Go Through Self-Play](https://www.science.org/doi/10.1126/science.aar6404), *Science*, 2018  
- Wu et al., [Accelerating Self-Play Learning in Go (KataGo)](https://arxiv.org/abs/1902.10565), arXiv, 2019  
- He et al., [Deep Residual Learning for Image Recognition (ResNet)](https://arxiv.org/abs/1512.03385), CVPR, 2016  
- Howard et al., [MobileNets: Efficient CNNs for Mobile Vision Applications](https://arxiv.org/abs/1704.04867), arXiv, 2017  
- Sandler et al., [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381), CVPR, 2018  
- Szegedy et al., [Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/abs/1512.00567), CVPR, 2016  
- Hu et al., [Squeeze-and-Excitation Networks](https://arxiv.org/abs/1709.01507), CVPR, 2018  
- Woo et al., [CBAM: Convolutional Block Attention Module](https://arxiv.org/abs/1807.06521), ECCV, 2018  
- Wang et al., [Linformer: Self-Attention with Linear Complexity](https://arxiv.org/abs/2006.04768), arXiv, 2020  
- Li et al., [EfficientFormer: Vision Transformers at MobileNet Speed](https://arxiv.org/abs/2206.01191), arXiv, 2022  
- Chen et al., [Dual Path Networks](https://arxiv.org/abs/1707.01629), NeurIPS, 2017  
- Cazenave, [Residual Networks for Computer Go](https://ieeexplore.ieee.org/document/8917762), *IEEE Transactions on Games*, 2020  
- Cazenave, [Mobile Networks for Computer Go](https://arxiv.org/abs/2008.10080), arXiv, 2020  
- Cazenave et al., [Cosine Annealing, MixNet and Swish Activation for Computer Go](https://arxiv.org/abs/2102.03467), arXiv, 2021  
- Sagri et al., [Vision Transformers for Computer Go](https://arxiv.org/abs/2309.12675), arXiv, 2023  
- KataGo by David J. Wu â€” [GitHub](https://github.com/lightvector/KataGo)

---
